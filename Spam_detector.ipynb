{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam_detector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgW8ZasfZHrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4af59b-3ae9-429b-b56f-97f71f8bc3ab"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asHKN-eLFGGb"
      },
      "source": [
        "Preprocessing step:\n",
        " Removing the extra characters, saving the words of the email in the dictionary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKqjqR80DDr_"
      },
      "source": [
        "def make_Dictionary(train_dir):\n",
        "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
        "    all_words = []       \n",
        "    for mail in emails:    \n",
        "        with open(mail) as m:\n",
        "            for i,line in enumerate(m):\n",
        "                if i == 2:  #Body of email is only 3rd line of text file\n",
        "                    words = line.split()\n",
        "                    all_words += words\n",
        "    \n",
        "    dictionary = Counter(all_words)\n",
        "    # code for non-word removal here \n",
        "    # removing extra characters: ex \",\"\n",
        "    list_to_remove = dictionary.keys()\n",
        "    for item in list(list_to_remove):\n",
        "      if item.isalpha() == False: \n",
        "        dictionary.pop(item)\n",
        "      elif len(item) == 1:\n",
        "        dictionary.pop(item)\n",
        "    dictionary = dictionary.most_common(3000)\n",
        "    print(len(dictionary))\n",
        "    \n",
        "    return dictionary"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBlBYWoQLVAt"
      },
      "source": [
        "Feature Extractions:\n",
        "\n",
        "Once the dictionary is ready, we can extract word count vector (our feature here) of 3000 dimensions for each email of training set. Each word count vector contains the frequency of 3000 words in the training file. The next code will generate a feature vector matrix whose rows denote 700 files of training set and columns denote 3000 words of dictionary. The value at index ‘ij’ will be the number of occurrences of jth word of dictionary in ith file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU6PbIrTLvu2"
      },
      "source": [
        "def extract_features(mail_dir): \n",
        "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
        "    features_matrix = np.zeros((len(files),3000))\n",
        "    docID = 0;\n",
        "    for fil in files:\n",
        "      with open(fil) as fi:\n",
        "        for i,line in enumerate(fi):\n",
        "          if i == 2:\n",
        "            words = line.split()\n",
        "            for word in words:\n",
        "              wordID = 0\n",
        "              for i,d in enumerate(dictionary):\n",
        "                if d[0] == word:\n",
        "                  wordID = i\n",
        "                  features_matrix[docID,wordID] = words.count(word)\n",
        "        docID = docID + 1     \n",
        "    return features_matrix"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFNqPkWa6QQW"
      },
      "source": [
        "We have trained two models here namely Naive Bayes classifier and Support Vector Machines (SVM). Naive Bayes classifier is a conventional and very popular method for document classification problem. It is a supervised probabilistic classifier based on Bayes theorem assuming independence between every pair of features. SVMs are supervised binary classifiers which are very effective when you have higher number of features. The goal of SVM is to separate some subset of training data from rest called the support vectors (boundary of separating hyper-plane). The decision function of SVM model that predicts the class of the test data is based on support vectors and makes use of a kernel trick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKnnlf8sQxy1"
      },
      "source": [
        "Create a dictionary of words with its frequency:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z74Owo9QluN",
        "outputId": "b07378ac-f0d0-45cc-ce0d-07f671b02d64"
      },
      "source": [
        "# Create a dictionary \n",
        "train_dir='/content/gdrive/MyDrive/ling-spam/train-mails/'\n",
        "test_dir='/content/gdrive/MyDrive/ling-spam/test-mails/'\n",
        "dictionary = make_Dictionary(train_dir)\n",
        "#print(dictionary)\n",
        "# removing extra characters: ex"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF3kFGTaQ3VA"
      },
      "source": [
        "Prepare feature vectors per training mail and its labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGDk8tQkQpfV"
      },
      "source": [
        "# feature vectors\n",
        "\n",
        "train_labels = np.zeros(702)\n",
        "train_labels[351:701] = 1\n",
        "train_matrix = extract_features(train_dir)\n",
        "#print(dictionary)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k78go5DrRCe6"
      },
      "source": [
        "Train and Testing: SVM and Naive bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpmNGcWq6SFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12d5818-e8f0-456c-a474-6913531e8dcc"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# Training SVM and Naive bayes classifier\n",
        "\n",
        "model1 = MultinomialNB()\n",
        "model2 = LinearSVC()\n",
        "model1.fit(train_matrix,train_labels)\n",
        "model2.fit(train_matrix,train_labels)\n",
        "\n",
        "# Test the unseen mails for Spam\n",
        "test_matrix = extract_features(test_dir)\n",
        "test_labels = np.zeros(260)\n",
        "test_labels[130:260] = 1\n",
        "result1 = model1.predict(test_matrix)\n",
        "result2 = model2.predict(test_matrix)\n",
        "#print(confusion_matrix(test_labels,result1))\n",
        "#print(confusion_matrix(test_labels,result2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV1vtA1scN5z"
      },
      "source": [
        "Conclusion:\n",
        "Both approach have a similar performance with accuracy more than %95, while SVD is a bit better. "
      ]
    }
  ]
}